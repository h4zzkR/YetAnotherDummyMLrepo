{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dls_2019_chatbot.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BfNSZLbJ5VSF"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OG9ja9uN5VSn"},"source":["\n","Chatbot Creating\n","================\n","**Baseline**: <br>`Matthew Inkawhich <https://github.com/MatthewInkawhich>` <br>\n","**Implementations and russian adaptation**: <br>\n","**@h4zzkR** (Telegram)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yHwjm0sm5VSx"},"source":["Preparations\n","------------\n","\n"]},{"cell_type":"code","metadata":{"id":"IRofiW7f5VS4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620337258323,"user_tz":-180,"elapsed":45492,"user":{"displayName":"GooglodictMicrobatch","photoUrl":"","userId":"04946338965592977091"}},"outputId":"0e9ccf4d-8787-4684-ccf5-ee69c251f8ec"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import unicode_literals\n","\n","import torch\n","from torch.jit import script, trace\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import csv\n","import random\n","import re\n","import os\n","import unicodedata\n","import codecs\n","from io import open\n","import itertools\n","import math\n","\n","from tqdm import tqdm\n","import pandas as pd\n","\n","from torch.utils.data import TensorDataset, DataLoader, Dataset\n","\n","colab = False\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    colab = True\n","except: \n","    pass\n","\n","\n","USE_CUDA = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","!pip install pymorphy2[fast]\n","!pip install -U pymorphy2-dicts-ru\n","\n","import pymorphy2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Collecting pymorphy2[fast]\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n","\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n","\u001b[K     |████████████████████████████████| 8.2MB 43.6MB/s \n","\u001b[?25hCollecting dawg-python>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n","Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n","Collecting DAWG>=0.8; extra == \"fast\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/ef/91b619a399685f7a0a95a03628006ba814d96293bbbbed234ee66fbdefd9/DAWG-0.8.0.tar.gz (371kB)\n","\u001b[K     |████████████████████████████████| 378kB 46.3MB/s \n","\u001b[?25hBuilding wheels for collected packages: DAWG\n","  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for DAWG: filename=DAWG-0.8.0-cp37-cp37m-linux_x86_64.whl size=868306 sha256=28ad7086750532bd125d75ea6c6fe3cb14cc1f96820bb784ff626135c136f823\n","  Stored in directory: /root/.cache/pip/wheels/3d/1f/f0/a5b1f9d02e193c997d252c33d215f24dfd7a448bc0166b2a12\n","Successfully built DAWG\n","Installing collected packages: pymorphy2-dicts-ru, dawg-python, DAWG, pymorphy2\n","Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n","Requirement already up-to-date: pymorphy2-dicts-ru in /usr/local/lib/python3.7/dist-packages (2.4.417127.4579844)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SZScfdfB5VTP"},"source":["Load & Preprocess Data\n","----------------------\n"]},{"cell_type":"code","metadata":{"id":"J_TM34U_6EOt","colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"status":"ok","timestamp":1562058154889,"user_tz":-180,"elapsed":29752,"user":{"displayName":"Googlodict Microbatch","photoUrl":"","userId":"04946338965592977091"}},"outputId":"2c609235-d94c-43d0-cbc4-d4d59f1c8e8a"},"source":["!cp /content/drive/My\\ Drive/dl_project/TlkPersonaChatRus.zip /content\n","#upload from toloka datasets page: \n","# https://yandex.ru/blog/toloka/toloka-datasets\n","!unzip TlkPersonaChatRus.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  TlkPersonaChatRus.zip\n","   creating: TlkPersonaChatRus/\n","  inflating: TlkPersonaChatRus/dialogues.tsv  \n","   creating: __MACOSX/\n","   creating: __MACOSX/TlkPersonaChatRus/\n","  inflating: __MACOSX/TlkPersonaChatRus/._dialogues.tsv  \n","  inflating: TlkPersonaChatRus/readme_TlkPersonaChatRus.txt  \n","  inflating: __MACOSX/TlkPersonaChatRus/._readme_TlkPersonaChatRus.txt  \n","  inflating: TlkPersonaChatRus/profiles.tsv  \n","  inflating: __MACOSX/TlkPersonaChatRus/._profiles.tsv  \n","  inflating: __MACOSX/._TlkPersonaChatRus  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LEwfImXPY9VR"},"source":["## Toloka dialoges"]},{"cell_type":"code","metadata":{"id":"Ot2r2AJ-5VU2"},"source":["# Default word tokens\n","PAD_token = 0  # Used for padding short sentences\n","SOS_token = 1  # Start-of-sentence token\n","EOS_token = 2  # End-of-sentence token\n","\n","class Voc:\n","    def __init__(self, name):\n","        self.name = name\n","        self.trimmed = False\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3  # Count SOS, EOS, PAD\n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.num_words\n","            self.word2count[word] = 1\n","            self.index2word[self.num_words] = word\n","            self.num_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    # Remove words below a certain count threshold\n","    def trim(self, min_count):\n","        if self.trimmed:\n","            return\n","        self.trimmed = True\n","\n","        keep_words = []\n","\n","        for k, v in self.word2count.items():\n","            if v >= min_count:\n","                keep_words.append(k)\n","\n","        print('keep_words {} / {} = {:.4f}'.format(\n","            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n","        ))\n","\n","        # Reinitialize dictionaries\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","        self.num_words = 3 # Count default tokens\n","\n","        for word in keep_words:\n","            self.addWord(word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLucs4bA5VVD"},"source":["Now we can assemble our vocabulary and query/response sentence pairs.\n","Before we are ready to use this data, we must perform some\n","preprocessing.\n","\n","First, we must convert the Unicode strings to ASCII using\n","``unicodeToAscii``. Next, we should convert all letters to lowercase and\n","trim all non-letter characters except for basic punctuation\n","(``normalizeString``). Finally, to aid in training convergence, we will\n","filter out sentences with length greater than the ``MAX_LENGTH``\n","threshold (``filterPairs``).\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"8wABy1eMtdZu","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1562063874942,"user_tz":-180,"elapsed":985,"user":{"displayName":"Googlodict Microbatch","photoUrl":"","userId":"04946338965592977091"}},"outputId":"cc082f61-6e26-416f-c007-6732942ca871"},"source":["from nltk.corpus import stopwords\n","import nltk\n","nltk.download('stopwords')\n","# filtered_words = [word for word in word_list if word not in stopwords.words('russian')]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"PImazuue6wpK"},"source":["def join(chat):\n","  i = 0\n","  new_chat = []\n","  from_ = chat[0][0]\n","  while True:\n","    line = [from_, '']\n","    try:\n","      while chat[i][0] == from_:\n","        line[1] += ' ' + chat[i][1] + ' '\n","        i += 1\n","    except: \n","      new_chat.append(line); break\n","    from_ = chat[i][0]\n","    new_chat.append(line)\n","  return new_chat\n","\n","def get_user(s):\n","  return s[s.find('class=')+6:s.find('class=')+19]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKYMCtWUwWA8"},"source":["morph = pymorphy2.MorphAnalyzer()\n","\n","morph_list = [\n","    'PAD', 'NOUN', 'ADJF', 'ADJS', 'COMP',\n","    'VERB', 'INFN', 'PRTF', 'PRTS',\n","    'GRND', 'NUMR', 'ADVB', 'NPRO',\n","    'PRED', 'PREP', 'CONJ', 'PRCL',\n","    'INTJ', 'sing', 'plur', 'masc',\n","    'femn', 'neut', 'LATN', 'PNCT',\n","    'NUMB', 'intg', 'real', 'UNKN', 'ROMN'\n","]\n","\n","morph2int = {m : i for i, m in enumerate(morph_list)}\n","del morph2int['PAD']\n","\n","int2morph = {i : m for i, m in enumerate(morph_list)}\n","del int2morph[0]\n","\n","MAX_LEN = len(morph2int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oPS2cBiwRR-J"},"source":["def morpher(word, leng):\n","  \n","  stop = 'VERB INFN GRND NUMR ADVB PRED NPRO PREP COMP CONJ PRCL INTJ'.split()\n","  data = morph.parse(word)[0]\n","  leng += 1\n","  gender, pos, number = 0, 0, 0\n","  pos = data.tag.POS\n","  if pos not in stop and pos != None:\n","    pos = morph2int[pos]\n","    gender = data.tag.gender\n","    number = data.tag.number\n","    try:\n","      gender = morph2int[gender]\n","    except: gender = 0\n","    try:\n","      number = morph2int[number]\n","    except: number = 0\n","  else:\n","    pos = str(list(data.tag.grammemes)[0])\n","    try:\n","      pos = morph2int[pos]\n","    except: pos = morph2int[data.tag.POS]\n","  \n","  return pos, gender, number\n","\n","\n","def get_morph_list(pair):\n","    m_pairs = []\n","    inp, out = pair[0], pair[1]\n","    m_inp = [morpher(i, MAX_LEN) for i in inp.split()]\n","    m_out = [morpher(i, MAX_LEN) for i in out.split()]\n","    print(m_inp, m_out); sys.exit()\n","    fts = torch.zeros(2, MAX_LEN + 1)\n","    fts[0][0] = m_inp[0]; fts[0][1] = m_inp[1]; fts[0][2] = m_inp[2]\n","    fts[1][0] = m_out[0]; fts[1][1] = m_out[1]; fts[1][2] = m_out[2]\n","      \n","    return fts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHu3kb7P5VVG","colab":{"base_uri":"https://localhost:8080/","height":434},"executionInfo":{"status":"error","timestamp":1562065889323,"user_tz":-180,"elapsed":3442,"user":{"displayName":"Googlodict Microbatch","photoUrl":"","userId":"04946338965592977091"}},"outputId":"342f0a3e-3535-4373-a779-4aa8dc41516c"},"source":["import sys\n","from nltk.tokenize import RegexpTokenizer\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","MAX_LENGTH = 15  # Maximum sentence length to consider\n","CONTEX_WINDOW = 15\n","\n","# Lowercase, trim, and remove non-letter characters\n","def normalizeString(s):\n","  s = re.sub(r\"([.!?()_;,:+=-])\", r\" \\1\", s.lower())\n","  s = re.sub('([.,!?()])', r' \\1 ', s)\n","  s = re.sub('\\s{2,}', ' ', s)\n","  s = re.sub(r\"[^а-яА-яa-zA-Z.!_;,:+=-?)(,й0123456789]+\", r\" \", s)\n","  s = re.sub(r\"\\s+\", r\" \", s).strip()\n","  s = s.replace('br /', ' ').replace('br', ' ')\n","  s = s.replace('>', ' ').replace('<', ' ').strip()\n","  return s\n","\n","def memory_preproc(mem):\n","  mem = ' '.join(mem).split()\n","  mem = mem[-CONTEX_WINDOW:len(mem)]\n","  mem = ' '.join(mem)\n","  mem = normalizeString(mem)\n","  mem = tokenizer.tokenize(mem)\n","  mem = [word for word in mem if word not in stopwords.words('russian')]\n","  mem = list(dict.fromkeys(mem))\n","  return ' '.join(mem)\n","  \n","  \n","# Read query/response pairs and return a voc object\n","def readVocs(datafile, corpus_name):\n","    print(\"Reading lines...\")\n","    toloka = pd.read_csv('TlkPersonaChatRus/dialogues.tsv', sep='\\t')\n","    test_pairs = []\n","    for dlg in tqdm(toloka['dialogue']):\n","      dlg = dlg.split(sep='</span><br />')\n","      n_dlg = [[get_user(d), d[d.find(':')+1:].strip()] for d in dlg][:-1]\n","      n_dlg = [[d[0], normalizeString(d[1])] for d in n_dlg]\n","      if len(n_dlg) % 2 != 0:\n","        n_dlg = n_dlg[:-1]\n","      n_dlg = join(n_dlg)\n","      for i in range(0, len(n_dlg)-1, 2):\n","        data = [n_dlg[i][1], n_dlg[i+1][1]]\n","        data = [normalizeString(d) for d in data]\n","#         fts = get_morph_list(data)\n","#         print(fts); sys.exit()\n","        memory = [d[1] for d in n_dlg[:i+1]]\n","        memory = memory_preproc(memory)\n","        test_pairs.append([data, memory])\n","        \n","    return test_pairs\n","\n","# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n","def filterPair(p):\n","    return len(p[0][0].split(' ')) < MAX_LENGTH and len(p[0][1].split(' ')) < MAX_LENGTH\n","\n","# Filter pairs using filterPair condition\n","def filterPairs(pairs):\n","    a = [pair for pair in pairs if filterPair(pair)]\n","    return a\n","  \n","  \n","def loadPrepareData(corpus, corpus_name, voc):\n","    print(\"Start preparing training data ...\")\n","    test_pairs = readVocs(corpus, corpus_name)\n","    print(\"Read {!s} sentence pairs\".format(len(test_pairs)))\n","    test_pairs = filterPairs(test_pairs)\n","    \n","    print(\"Trimmed to {!s} sentence pairs\".format(len(test_pairs)))\n","    print(\"Counting words...\")\n","    for pair in test_pairs:\n","        voc.addSentence(pair[0][0])\n","        voc.addSentence(pair[0][1])\n","        voc.addSentence(pair[1])\n","    print(\"Counted words:\", voc.num_words)\n","    return voc, test_pairs\n","\n","\n","corpus_name = 'xl_vocab'\n","voc = Voc(corpus_name)\n","\n","corpus = 'TlkPersonaChatRus/dialogues.tsv'\n","voc, toloka = loadPrepareData(corpus, 'toloka', voc)\n","print(\"\\npairs:\")\n","for pair in  toloka[:10]:\n","    print(pair[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Start preparing training data ...\n","Reading lines...\n"],"name":"stdout"},{"output_type":"stream","text":["  2%|▏         | 233/10013 [00:02<01:31, 106.32it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-e4ce3cf0cfed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'TlkPersonaChatRus/dialogues.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoloka\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadPrepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toloka'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\npairs:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mtoloka\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-e4ce3cf0cfed>\u001b[0m in \u001b[0;36mloadPrepareData\u001b[0;34m(corpus, corpus_name, voc)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadPrepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start preparing training data ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mtest_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadVocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read {!s} sentence pairs\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mtest_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilterPairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-e4ce3cf0cfed>\u001b[0m in \u001b[0;36mreadVocs\u001b[0;34m(datafile, corpus_name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#         print(fts); sys.exit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_dlg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_preproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtest_pairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-e4ce3cf0cfed>\u001b[0m in \u001b[0;36mmemory_preproc\u001b[0;34m(mem)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'russian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-e4ce3cf0cfed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmem\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'russian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, fileid)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such file or directory: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/posixpath.py\u001b[0m in \u001b[0;36mnormpath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    364\u001b[0m         if (comp != dotdot or (not initial_slashes and not new_comps) or\n\u001b[1;32m    365\u001b[0m              (new_comps and new_comps[-1] == dotdot)):\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mnew_comps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnew_comps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mnew_comps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"H5qZI6H7nX8H"},"source":["## Github dataset"]},{"cell_type":"code","metadata":{"id":"MekOTNsYiiA7"},"source":["# def read_text(replies):\n","#   replies = [normalizeString(r) for r in replies]\n","#   if len(replies) % 2 != 0:\n","#     replies = replies[:-1]\n","#   pairs = []\n","#   for i in range(0, len(replies)-1, 2):\n","#     data = [replies[i], replies[i+1]]\n","#     memory = [d for d in replies[:i+1]]\n","#     memory = memory_preproc(memory)\n","#     pairs.append([data, memory])\n","#   return pairs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpT_ZaNdigQR"},"source":["# with open('dialogues.txt') as file:\n","#   f = file.read().split('\\n\\n')\n","#   dataset = []\n","#   for dialog in tqdm(f):\n","#     replies = dialog.split('\\n')\n","#     replies = read_text(replies)\n","#     dataset.append(replies)\n","\n","# def filter_Pair(p):\n","#     try:\n","#       return len(p[0][0][0].split(' ')) < MAX_LENGTH and len(p[0][0][1].split(' ')) < MAX_LENGTH\n","#     except IndexError:\n","#       return False\n","\n","# # Filter pairs using filterPair condition\n","# def filter_Pairs(pairs):\n","#     a = [pair for pair in pairs if filter_Pair(pair)]\n","#     return a\n","\n","# github = []\n","# for d in dataset:\n","#   try:\n","#     github.append(d[0])\n","#   except IndexError: continue\n","    \n","# github = filter_Pairs(github)\n","# print(\"Trimmed to {!s} sentence pairs\".format(len(github)))\n","# print(\"Counting words...\")\n","\n","# for pair in github:\n","#   voc.addSentence(' '.join(pair[0][0]))\n","#   voc.addSentence(' '.join(pair[0][1]))\n","#   voc.addSentence(' '.join(pair[1]))\n","    \n","# print(\"Counted words:\", voc.num_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"47FC0EI25VVU"},"source":["**Trim rare words**"]},{"cell_type":"code","metadata":{"id":"_DidIpB_p_cn"},"source":["# combined = toloka + github\n","combined = toloka"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wF5LkdPy5VVY","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1562064351193,"user_tz":-180,"elapsed":851,"user":{"displayName":"Googlodict Microbatch","photoUrl":"","userId":"04946338965592977091"}},"outputId":"7cc29c7a-d959-4562-aac6-2db29a0ba5f9"},"source":["MIN_COUNT = 3    # Minimum word count threshold for trimming\n","\n","def trimRareWords(voc, pairs, morph_pairs, MIN_COUNT):\n","    voc.trim(MIN_COUNT)\n","    keep_pairs = []\n","    for i in range(len(pairs)):\n","        pair = pairs[i]\n","        input_sentence = pair[0][0]\n","        output_sentence = pair[0][1]\n","        memory = pair[1]\n","        keep_input = True\n","        keep_output = True\n","        keep_memory = True\n","\n","        for word in input_sentence.split(' '):\n","            if word not in voc.word2index or word == 'готовить':\n","                keep_input = False\n","                break\n","        for word in output_sentence.split(' '):\n","            if word not in voc.word2index or word == 'готовить':\n","                keep_output = False\n","                break\n","                \n","        for word in memory.split(' '):\n","          if word not in voc.word2index or word == 'готовить':\n","              keep_memory = False\n","              break\n","\n","        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n","        if keep_input and keep_output and keep_memory:\n","            keep_pairs.append(pair)\n","\n","    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n","    return keep_pairs\n","\n","\n","dataset = trimRareWords(voc, combined, morph_pairs, MIN_COUNT)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Trimmed from 49622 pairs to 34811, 0.7015 of total\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WMUXwHcM-TH_","colab":{"base_uri":"https://localhost:8080/","height":799},"executionInfo":{"status":"ok","timestamp":1562064353463,"user_tz":-180,"elapsed":613,"user":{"displayName":"Googlodict Microbatch","photoUrl":"","userId":"04946338965592977091"}},"outputId":"65bf8dc3-66f3-4e65-87e3-1dad36cfd6a4"},"source":["dataset[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['привет ) расскажи о себе',\n","  'привет ) под вкусный кофеек настроение поболтать появилось )'],\n"," 'привет расскажи',\n"," ([tensor([ 1., 21., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([24.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([28.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.])],\n","  [tensor([ 1., 21., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([24.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([ 1., 21., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([14.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([ 2., 21., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([ 1., 21., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([ 1., 21., 18.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.]),\n","   tensor([24.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n","            0.,  0.])])]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"pTkGtg9p5VVt"},"source":["Prepare Data for Models\n","-----------------------\n"]},{"cell_type":"code","metadata":{"id":"7ngzYoLsuV_D"},"source":["MAX_LENGTH = max([len(i[0][0].split()) for i in dataset])\n","MAX_LENGTH = max([MAX_LENGTH] + [len(i[0][1].split()) for i in dataset])\n","MAX_LENGTH = MAX_LENGTH + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CAboEkrQ5VVx","colab":{"base_uri":"https://localhost:8080/","height":903},"outputId":"67616dd9-bf1e-4415-edc0-bbec581f1bee"},"source":["def indexesFromSentence(voc, sentence):\n","    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n","\n","def zeroPadding(l, fillvalue=PAD_token):\n","    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n","\n","def binaryMatrix(l, value=PAD_token):\n","    m = []\n","    for i, seq in enumerate(l):\n","        m.append([])\n","        for token in seq:\n","            if token == PAD_token:\n","                m[i].append(0)\n","            else:\n","                m[i].append(1)\n","    return m\n","\n","def inputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, lengths\n","  \n","def memoryVar(mem, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in mem]\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, lengths\n","\n","def outputVar(l, voc):\n","    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n","    max_target_len = max([len(indexes) for indexes in indexes_batch])\n","    padList = zeroPadding(indexes_batch)\n","    mask = binaryMatrix(padList)\n","    mask = torch.ByteTensor(mask)\n","    padVar = torch.LongTensor(padList)\n","    return padVar, mask, max_target_len\n","\n","def batch2TrainData(voc, pair_batch):\n","    pair_batch.sort(key=lambda x: len(x[0][0].split(\" \")), reverse=True)\n","    input_batch, memory_batch, output_batch = [], [], []\n","    for pair in pair_batch:\n","        input_batch.append(pair[0][0])\n","        output_batch.append(pair[0][1])\n","        memory_batch.append(pair[1])\n","    inp, lengths = inputVar(input_batch, voc)\n","    mem, mem_lengths = memoryVar(memory_batch, voc)\n","    output, mask, max_target_len = outputVar(output_batch, voc)\n","    return inp, lengths, mem, mem_lengths, output, mask, max_target_len\n","\n","\n","# Example for validation\n","small_batch_size = 5\n","batches = batch2TrainData(voc, [random.choice(dataset) for _ in range(small_batch_size)])\n","input_variable, lengths, mem, mem_lengths, target_variable, mask, max_target_len = batches\n","\n","print(\"input_variable:\", input_variable)\n","print(\"lengths:\", lengths)\n","print('memory: ', mem)\n","print('memory lens: ', mem_lengths)\n","print(\"target_variable:\", target_variable)\n","print(\"mask:\", mask)\n","print(\"max_target_len:\", max_target_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["input_variable: tensor([[  103,  4501,  3476,   569,     3],\n","        [ 4315,    25,    45,  5777,     2],\n","        [ 1114,    17,   103,     2,     0],\n","        [   25,    21,  1029,     0,     0],\n","        [   56,     4,  1083,     0,     0],\n","        [   57,     4,     2,     0,     0],\n","        [ 1379,     2,     0,     0,     0],\n","        [15292,     0,     0,     0,     0],\n","        [ 3352,     0,     0,     0,     0],\n","        [   29,     0,     0,     0,     0],\n","        [ 6566,     0,     0,     0,     0],\n","        [   78,     0,     0,     0,     0],\n","        [ 1027,     0,     0,     0,     0],\n","        [    4,     0,     0,     0,     0],\n","        [    2,     0,     0,     0,     0]])\n","lengths: tensor([15,  7,  6,  3,  2])\n","memory:  tensor([[  103,   282,   113,  1643,     3],\n","        [ 4315,   102,   139,  1706,     2],\n","        [ 1114,   283,   140,   509,     0],\n","        [ 1379,  3716,   830,   248,     0],\n","        [15292,  4501,  3476,  3231,     0],\n","        [ 3352,     2,   103,   569,     0],\n","        [ 6566,     0,  1029,  5777,     0],\n","        [    2,     0,  1083,     2,     0],\n","        [    0,     0,     2,     0,     0]])\n","memory lens:  tensor([8, 6, 9, 8, 2])\n","target_variable: tensor([[  33,    5, 1549, 2410,    3],\n","        [ 296,   14,   41,    2,   45],\n","        [ 597,  102,  322,    0,   51],\n","        [2203,  441,  302,    0,  144],\n","        [ 296,   25,   97,    0,    2],\n","        [3358,   14,   71,    0,    0],\n","        [  25,  219, 4220,    0,    0],\n","        [  17, 4031,   16,    0,    0],\n","        [ 583,    2,    2,    0,    0],\n","        [ 267,    0,    0,    0,    0],\n","        [ 969,    0,    0,    0,    0],\n","        [   2,    0,    0,    0,    0]])\n","mask: tensor([[1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1],\n","        [1, 1, 1, 0, 1],\n","        [1, 1, 1, 0, 1],\n","        [1, 1, 1, 0, 1],\n","        [1, 1, 1, 0, 0],\n","        [1, 1, 1, 0, 0],\n","        [1, 1, 1, 0, 0],\n","        [1, 1, 1, 0, 0],\n","        [1, 0, 0, 0, 0],\n","        [1, 0, 0, 0, 0],\n","        [1, 0, 0, 0, 0]], dtype=torch.uint8)\n","max_target_len: 12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YcolBqxw5VV_"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"nFHTi4YFkFoI"},"source":["**Pipeline**\n","--------------------\n","**Input_data** is reply ('Hello') and 'memory'.<br>\n","**Memory** is fixed length previous user and bot replyes without stopwords.<br>\n","\n","-----------------------\n","Special encoder (general encoder = gencoder) is responsible for detecting dialog context, with help of it<br>\n","seq2seq model is trying to create message. Hidden state from gencoder is using for encoder rnn.<br>\n","In the decoder section, we count attention memory weights and attention context weights (gencoder_outputs and encoder_outputs). <br>\n","After that, we concat rnn_output, context, memory_context (hiddensize*3) and push it to linear.\n"]},{"cell_type":"code","metadata":{"id":"RFdqPbCGvG9F"},"source":["class PositionalEmbedding(nn.Module):\n","  \"\"\"\n","  embedding that consider \n","  words position in sentence\n","  \"\"\"\n","  def __init__(self, emb_dim, max_seq_len):\n","      super().__init__()\n","      self.emb_dim = emb_dim\n","\n","      pe = torch.zeros(max_seq_len, emb_dim)\n","      for pos in range(max_seq_len):\n","          for i in range(0, emb_dim, 2):\n","              pe[pos, i] = \\\n","              math.sin(pos / (10000 ** ((2 * i)/emb_dim)))\n","              pe[pos, i + 1] = \\\n","              math.cos(pos / (10000 ** ((2 * (i + 1))/emb_dim)))\n","\n","      pe = pe.unsqueeze(0)\n","      self.register_buffer('pe', pe)\n","        \n","  def forward(self, x):\n","      x = x * math.sqrt(self.emb_dim)\n","      #add constant to embedding\n","      seq_len = x.shape[0]\n","      x = x.permute(1, 0, 2)\n","      x = x + Variable(self.pe[:,:seq_len],\n","                       requires_grad=False)\n","      return x\n","      \n","class Embedding(nn.Module):\n","  def __init__(self, vocab_len, emb_dim, max_seq_len=100):\n","    super(Embedding, self).__init__()\n","    self.emb_dim = emb_dim\n","    self.v_len = vocab_len\n","    self.embedding = nn.Embedding(self.v_len, self.emb_dim, padding_idx=0)\n","    self.pos_encode = PositionalEmbedding(emb_dim, max_seq_len)\n","    \n","  def forward(self, x):\n","    b = self.embedding(x)\n","    return self.pos_encode(b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVqS3J_qyWjb"},"source":["class GlobalEncoder(nn.Module):\n","    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n","        super(GlobalEncoder, self).__init__()\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.embedding = embedding\n","\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n","                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n","        \n","\n","\n","    def forward(self, input_seq, input_lengths, hidden=None):\n","        embedded = self.embedding(input_seq)\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n","        outputs, hidden = self.gru(packed, hidden)\n","\n","        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n","        return outputs, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oV2uTBKW5VWG"},"source":["class Encoder(nn.Module):\n","    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n","        super(Encoder, self).__init__()\n","        self.n_layers = n_layers\n","        self.hidden_size = hidden_size\n","        self.embedding = embedding\n","\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n","                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n","\n","    def forward(self, input_seq, input_lengths, memory_out=0, memory_hidden=None):\n","\n","        embedded = self.embedding(input_seq)\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n","        outputs, hidden = self.gru(packed, memory_hidden)\n","        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n","\n","        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n","        return outputs, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2Yw5YR05VWd"},"source":["# Luong attention layer\n","class Attn(torch.nn.Module):\n","    def __init__(self, method, hidden_size):\n","        super(Attn, self).__init__()\n","        self.method = method\n","\n","        self.hidden_size = hidden_size\n","        self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n","        \n","    def dot_score(self, hidden, encoder_output):\n","        return torch.sum(hidden * encoder_output, dim=2)\n","\n","    def general_score(self, hidden, encoder_output):\n","        energy = self.attn(encoder_output)\n","        return torch.sum(hidden * energy, dim=2)\n","\n","    def forward(self, hidden, encoder_outputs, mask): #+mask\n","        if self.method == 'general':\n","            attn_energies = self.general_score(hidden, encoder_outputs)\n","        elif self.method == 'dot':\n","            attn_energies = self.dot_score(hidden, encoder_outputs)\n","\n","        attn_energies = attn_energies.t()\n","        #masked attention\n","        attn_energies[~mask] = float('-inf')\n","\n","        return F.softmax(attn_energies, dim=1).unsqueeze(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cx39naRS5VWu"},"source":["class Decoder(nn.Module):\n","    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n","        super(Decoder, self).__init__()\n","\n","        # Keep for reference\n","        self.attn_model = attn_model\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        # Define layers\n","        self.embedding = embedding\n","        self.embedding_dropout = nn.Dropout(dropout)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n","        \n","        self.dense_bn = nn.BatchNorm1d(hidden_size)\n","        self.concat = nn.Linear(hidden_size * 3, hidden_size)\n","        \n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","        self.attn = Attn(attn_model, hidden_size)\n","\n","    def forward(self, input_step, last_hidden, encoder_outputs, memory_out, memory_hidden,\n","                                                                memory_mask, encoder_mask):\n","\n","        embedded = self.embedding(input_step)\n","        embedded = self.embedding_dropout(embedded)\n","        embedded = embedded.permute(1, 0, 2)\n","\n","        rnn_output, hidden = self.gru(embedded, last_hidden)\n","\n","        attn_weights = self.attn(rnn_output, encoder_outputs, encoder_mask)\n","        memory_weights = self.attn(rnn_output, memory_out, memory_mask)\n","\n","        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n","        memory_context = memory_weights.bmm(memory_out.transpose(0, 1))\n","\n","        rnn_output = rnn_output.squeeze(0)\n","        \n","        context = context.squeeze(1)\n","        memory_context = memory_context.squeeze(1)\n","\n","        concat_input = torch.cat((rnn_output, context, memory_context), 1)\n","        concat_output = torch.tanh(self.concat(concat_input))\n","        concat_output = self.dense_bn(concat_output)\n","\n","        output = self.out(concat_output)\n","        output = F.softmax(output, dim=1)\n","\n","        return output, hidden"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hCNdxFB95VW6"},"source":["Define Training Procedure\n","-------------------------\n","\n","Masked loss\n","~~~~~~~~~~~\n","\n","Since we are dealing with batches of padded sequences, we cannot simply\n","consider all elements of the tensor when calculating loss. We define\n","``maskNLLLoss`` to calculate our loss based on our decoder’s output\n","tensor, the target tensor, and a binary mask tensor describing the\n","padding of the target tensor. This loss function calculates the average\n","negative log likelihood of the elements that correspond to a *1* in the\n","mask tensor.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"D2sJhxdo5VW-"},"source":["def maskNLLLoss(inp, target, mask):\n","    nTotal = mask.sum()\n","    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n","    loss = crossEntropy.masked_select(mask).mean()\n","    loss = loss.to(device)\n","    return loss, nTotal.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNDsCpIp5VXH"},"source":["def train(input_variable, lengths, memory, memory_lens, target_variable, mask, max_target_len, gencoder, encoder, decoder, embedding,\n","          gencoder_optimizer, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n","\n","    gencoder_optimizer.zero_grad()\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_variable = input_variable.to(device)\n","    lengths = lengths.to(device)\n","    memory = memory.to(device)\n","    memory_lens = memory_lens.to(device)\n","    target_variable = target_variable.to(device)\n","    mask = mask.to(device)\n","\n","    loss = 0\n","    print_losses = []\n","    n_totals = 0\n","\n","    memory_mask = memory > 0\n","    memory_mask = memory_mask.permute(1, 0).to(device)\n","    encoder_mask = input_variable > 0\n","    encoder_mask = encoder_mask.permute(1, 0).to(device)\n","\n","    gencoder_outputs, gencoder_hidden = gencoder(memory, memory_lens)\n","    encoder_outputs, encoder_hidden = encoder(input_variable, lengths,  gencoder_outputs, gencoder_hidden)\n","\n","    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n","    decoder_input = decoder_input.to(device)\n","\n","    decoder_hidden = encoder_hidden[:decoder.n_layers]\n","\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # Forward batch of sequences through decoder one time step at a time\n","    if use_teacher_forcing:\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs,\n","                gencoder_outputs, gencoder_hidden, memory_mask,\n","                encoder_mask\n","            )\n","            # Teacher forcing: next input is current target\n","            decoder_input = target_variable[t].view(1, -1)\n","            # Calculate and accumulate loss\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","    else:\n","        for t in range(max_target_len):\n","            decoder_output, decoder_hidden = decoder(\n","                decoder_input, decoder_hidden, encoder_outputs\n","            )\n","            # No teacher forcing: next input is decoder's own current output\n","            _, topi = decoder_output.topk(1)\n","            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n","            decoder_input = decoder_input.to(device)\n","            # Calculate and accumulate loss\n","            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n","            loss += mask_loss\n","            print_losses.append(mask_loss.item() * nTotal)\n","            n_totals += nTotal\n","\n","    # Perform backpropatation\n","    loss.backward()\n","\n","    # Clip gradients: gradients are modified in place\n","    _ = torch.nn.utils.clip_grad_norm_(gencoder.parameters(), clip)\n","    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n","    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n","\n","    # Adjust model weights\n","    gencoder_optimizer.step()\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return sum(print_losses) / n_totals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMValIFf5VXW"},"source":["def trainIters(model_name, voc, pairs, gencoder, encoder, decoder, gencoder_optimizer, encoder_optimizer, decoder_optimizer, \n","               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, \n","               print_every, save_every, clip, corpus_name, loadFilename):\n","\n","    # Load batches for each iteration\n","    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n","                      for _ in range(n_iteration)]\n","\n","    # Initializations\n","    print('Initializing ...')\n","    start_iteration = 1\n","    print_loss = 0\n","    if loadFilename:\n","        start_iteration = checkpoint['iteration'] + 1\n","\n","    # Training loop\n","    print(\"Training...\")\n","    for iteration in range(start_iteration, n_iteration + 1):\n","        training_batch = training_batches[iteration - 1]\n","        # Extract fields from batch\n","        input_variable, lengths, memory, memory_lens, target_variable, mask, max_target_len = training_batch\n","\n","        # Run a training iteration with batch\n","        loss = train(input_variable, lengths, memory, memory_lens, target_variable, mask, max_target_len, gencoder, encoder,\n","                     decoder, embedding, gencoder_optimizer, encoder_optimizer, decoder_optimizer, batch_size, clip)\n","        print_loss += loss\n","\n","        # Print progress\n","        if iteration % print_every == 0:\n","            print_loss_avg = print_loss / print_every\n","            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n","            print_loss = 0\n","\n","        # Save checkpoint\n","        if (iteration % save_every == 0):\n","            torch.save({\n","                'iteration': iteration,\n","                'gen' : gencoder.state_dict(),\n","                'en': encoder.state_dict(),\n","                'de': decoder.state_dict(),\n","                'gen_opt' : gencoder_optimizer.state_dict(),\n","                'en_opt': encoder_optimizer.state_dict(),\n","                'de_opt': decoder_optimizer.state_dict(),\n","                'loss': loss,\n","                'voc_dict': voc.__dict__,\n","                'embedding': embedding.state_dict()\n","            }, os.path.join('/content/', '{}_{}.tar'.format(iteration, 'checkpoint')))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uWqJIfHA5VX4"},"source":["Run Model\n","---------\n","\n","Finally, it is time to run our model!\n","\n","Regardless of whether we want to train or test the chatbot model, we\n","must initialize the individual encoder and decoder models. In the\n","following block, we set our desired configurations, choose to start from\n","scratch or set a checkpoint to load from, and build and initialize the\n","models. Feel free to play with different model configurations to\n","optimize performance.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"cPX3SP31Tln0"},"source":["# !cp /content/drive/My\\ Drive/10_checkpoint.tar /content/\n","!cp 1_checkpoint.tar /content/drive/My\\ Drive/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8W4Y3sWg5VX8"},"source":["# Configure models\n","model_name = 'cb_model'\n","# attn_model = 'dot'\n","attn_model = 'general'\n","#attn_model = 'concat'\n","\n","#small hiddens can`t be effecient in loss and predict\n","#for such a big vocabulary\n","\n","hidden_size = 512\n","encoder_n_layers = 3\n","decoder_n_layers = 2\n","dropout = 0.2\n","batch_size = 32\n","\n","# Set checkpoint to load from; set to None if starting from scratch\n","loadFilename = None\n","checkpoint_iter = 500\n","\n","# loadFilename = os.path.join('/content/2000_checkpoint.tar')\n","\n","\n","# Load model if a loadFilename is provided\n","if loadFilename:\n","    # If loading on same machine the model was trained on\n","    checkpoint = torch.load(loadFilename)\n","    # If loading a model trained on GPU to CPU\n","    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n","    encoder_sd = checkpoint['en']\n","    decoder_sd = checkpoint['de']\n","    gencoder_sd = checkpoint['gen']\n","    gencoder_optimizer_sd = checkpoint['gen_opt']\n","    encoder_optimizer_sd = checkpoint['en_opt']\n","    decoder_optimizer_sd = checkpoint['de_opt']\n","    embedding_sd = checkpoint['embedding']\n","    voc.__dict__ = checkpoint['voc_dict']\n","\n","\n","print('Building encoder and decoder ...')\n","# Initialize word embeddings\n","embedding = Embedding(voc.num_words, hidden_size)\n","\n","if loadFilename:\n","    embedding.load_state_dict(embedding_sd)\n","# Initialize encoder & decoder models\n","\n","gencoder = GlobalEncoder(hidden_size, embedding, encoder_n_layers, dropout)\n","encoder = Encoder(hidden_size, embedding, encoder_n_layers, dropout)\n","decoder = Decoder(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n","\n","if loadFilename:\n","    gencoder.load_state_dict(gencoder_sd)\n","    encoder.load_state_dict(encoder_sd)\n","    decoder.load_state_dict(decoder_sd)\n","# Use appropriate device\n","\n","gencoder = gencoder.to(device)\n","encoder = encoder.to(device)\n","decoder = decoder.to(device)\n","print('Models built and ready to go!')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OG1yj_K7mgb3"},"source":["start, end = 1500, 4500\n","for i in range(start, end + save_every, save_every):\n","  d = str(i) + '_checkpoint.tar'\n","  !rm $d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEB8m3nY5VYH","colab":{"base_uri":"https://localhost:8080/","height":260},"outputId":"888e1bdd-2072-4e07-a4b7-8455990ed1b6"},"source":["from torch.autograd import Variable\n","\n","clip = 50.0\n","teacher_forcing_ratio = 1.0\n","learning_rate = 0.0001\n","decoder_learning_ratio = 5.0\n","\n","#try 10000+, bcz of size of dataset\n","n_iteration = 1000\n","print_every = 100\n","save_every = 1000\n","\n","save_dir = '/content/'\n","corpus_name = 'corp'\n","\n","# Ensure dropout layers are in train mode\n","gencoder.train()\n","encoder.train()\n","decoder.train()\n","\n","# Initialize optimizers\n","print('Building optimizers ...')\n","gencoder_optimizer = optim.Adam(gencoder.parameters(), lr=learning_rate * 2.5)\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n","\n","if loadFilename:\n","    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n","    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n","\n","# Run training iterations\n","print(\"Starting Training!\")\n","trainIters(model_name, voc, dataset, gencoder, encoder, decoder, gencoder_optimizer, encoder_optimizer, decoder_optimizer,\n","           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n","           print_every, save_every, clip, corpus_name, loadFilename)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Building optimizers ...\n","Starting Training!\n","Initializing ...\n","Training...\n","Iteration: 100; Percent complete: 10.0%; Average loss: 1.7875\n","Iteration: 200; Percent complete: 20.0%; Average loss: 1.8306\n","Iteration: 300; Percent complete: 30.0%; Average loss: 1.8440\n","Iteration: 400; Percent complete: 40.0%; Average loss: 1.8327\n","Iteration: 500; Percent complete: 50.0%; Average loss: 1.7820\n","Iteration: 600; Percent complete: 60.0%; Average loss: 1.7731\n","Iteration: 700; Percent complete: 70.0%; Average loss: 1.7774\n","Iteration: 800; Percent complete: 80.0%; Average loss: 1.7179\n","Iteration: 900; Percent complete: 90.0%; Average loss: 1.7566\n","Iteration: 1000; Percent complete: 100.0%; Average loss: 1.7080\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dMr1-Kqn0DIe"},"source":["# Chatbot"]},{"cell_type":"markdown","metadata":{"id":"JxV_-X1H0QaM"},"source":["Define Evaluation\n","-----------------\n"]},{"cell_type":"code","metadata":{"id":"e2OU4tKg0EiH"},"source":["def evaluate(gencoder, encoder, decoder, searcher, voc, sentence, memory, max_length=MAX_LENGTH):\n","  \n","    memory_lengths = 0\n","    has_memory = False\n","    \n","    memory = [indexesFromSentence(voc, ' '.join(memory))]\n","    memory_lengths = torch.tensor([len(indexes) for indexes in memory])\n","    memory = torch.LongTensor(memory).transpose(0, 1)\n","    memory.to(device)\n","    memory_lengths.to(device)\n","    has_memory = True\n","      \n","    indexes_batch = [indexesFromSentence(voc, sentence)]\n","    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n","    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n","\n","    input_batch = input_batch.to(device)\n","    lengths = lengths.to(device)\n","    tokens, scores = searcher(input_batch, lengths, max_length, memory, memory_lengths, has_memory)\n","    decoded_words = [voc.index2word[token.item()] for token in tokens]\n","    return decoded_words\n","  \n","\n","class MemoryMechanism():\n","  def __init__(self):\n","    self.memory = []\n","    self.limit = CONTEX_WINDOW\n","      \n","  def append_input(self, input_seq):\n","    if len(self.memory) < self.limit-1:\n","      data = input_seq\n","      memory = [normalizeString(d) for d in data]\n","      memory = memory_preproc(memory)\n","      memory = [token for token in memory if token in voc.word2index.keys()]\n","      self.memory += memory\n","    else:\n","      self.memory.pop(0)\n","    \n","  def drop(self):\n","    self.memory = []\n","\n","def evaluateInput(encoder, decoder, searcher, voc):\n","    input_sentence = ''\n","    memory_class = MemoryMechanism()\n","    while(1):\n","        try:\n","            input_sentence = input('> ')\n","            if input_sentence == 'q' or input_sentence == 'quit': break\n","            if input_sentence == 'clear': memory_class.drop(); continue\n","            input_sentence = normalizeString(input_sentence)\n","            memory_class.append_input(input_sentence.split())\n","            memory = memory_class.memory\n","            output_words = evaluate(gencoder, encoder, decoder, searcher, voc, input_sentence, memory)\n","            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n","            print('Bot:', ' '.join(output_words))\n","            memory_class.append_input(output_words)\n","\n","        except KeyError:\n","            print(\"Error: Encountered unknown word.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7Y6hFx60ITi"},"source":["class GreedySearchDecoder(nn.Module):\n","    def __init__(self, gencoder, encoder, decoder):\n","        super(GreedySearchDecoder, self).__init__()\n","        self.gencoder = gencoder\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, input_seq, input_length, max_length, memory_seq, memory_lengths, has_memory=True):\n","      \n","        memory_seq, memory_lengths = memory_seq.to(device), memory_lengths.to(device)\n","        if has_memory:\n","          gencoder_outputs, gencoder_hidden = self.gencoder(memory_seq, memory_lengths)\n","        else:\n","          gencoder_outputs, gencoder_hidden = None, None\n","        \n","        gencoder_outputs, gencoder_hidden = gencoder_outputs.to(device), gencoder_hidden.to(device)\n","          \n","        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length, gencoder_outputs, gencoder_hidden)\n","        decoder_hidden = encoder_hidden[:decoder.n_layers]\n","        \n","        memory_mask = memory_seq > 0\n","        memory_mask = memory_mask.permute(1,0).to(device)\n","        \n","        encoder_mask = input_seq > 0\n","        encoder_mask = encoder_mask.permute(1, 0).to(device)\n","\n","        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n","        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n","        all_scores = torch.zeros([0], device=device)\n","        for _ in range(max_length):\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, \n","                                                          encoder_outputs, gencoder_outputs, gencoder_hidden,\n","                                                          memory_mask, encoder_mask)\n","            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n","            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n","            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n","            decoder_input = torch.unsqueeze(decoder_input, 0)\n","        return all_tokens, all_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xICwTi_qeVeg","colab":{"base_uri":"https://localhost:8080/","height":643},"outputId":"6709408f-5466-4473-a4e0-a0c28450656e"},"source":["# Set dropout layers to eval mode\n","gencoder.eval()\n","encoder.eval()\n","decoder.eval()\n","\n","# Initialize search module\n","searcher = GreedySearchDecoder(gencoder, encoder, decoder)\n","\n","# Begin chatting (uncomment and run the following line to begin)\n","evaluateInput(encoder, decoder, searcher, voc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["> приветик\n","Bot: привет )\n","> что делаешь?\n","Bot: люблю , а вы ?\n","> я тоже)\n","Bot: ооо , это круто\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-62f0e4756824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-36-29695aa63278>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"Z59vX33A4Uga"},"source":["# Exxtra"]},{"cell_type":"markdown","metadata":{"id":"HkTazXOnmudN"},"source":["**Telegram chat bot**: <br>\n","[Here you can find *it*](https://t.me/stupid_chat_arc4num_bot)"]},{"cell_type":"code","metadata":{"id":"aeIi2vou4T-Z"},"source":["!pip install python-telegram-bot --upgrade"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_LtVAAO4t9u"},"source":["def chat_input(input_seq, gencoder, encoder, decoder, searcher, voc):\n","    input_sentence = ''\n","    memory_class = MemoryMechanism()\n","    try:\n","        input_sentence = input_seq\n","        input_sentence = normalizeString(input_sentence)\n","        memory_class.append_input(input_sentence.split())\n","        memory = memory_class.memory\n","        output_words = evaluate(gencoder, encoder, decoder, searcher, voc, input_sentence, memory)\n","        output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n","        memory_class.append_input(output_words)\n","        response = ' '.join(output_words)\n","        return response\n","\n","    except KeyError:\n","        print(\"Error: Encountered unknown word.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHhLTqLi5a_t"},"source":["from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n","updater = Updater(token='877251641:AAGF32236-QIlbIjufWHXkBAEwG8DOX9M3U')\n","dispatcher = updater.dispatcher\n","\n","# Обработка команд\n","def startCommand(bot, update):\n","    bot.send_message(chat_id=update.message.chat_id, text='Привет, я чат-бот.')\n","    \n","def textMessage(bot, update):\n","    message = update.message.text\n","    response = chat_input(message, gencoder, encoder, decoder, searcher, voc)\n","    bot.send_message(chat_id=update.message.chat_id, text=response)\n","\n","\n","\n","start_command_handler = CommandHandler('start', startCommand)\n","text_message_handler = MessageHandler(Filters.text, textMessage)\n","dispatcher.add_handler(start_command_handler)\n","dispatcher.add_handler(text_message_handler)\n","updater.start_polling(clean=True)\n","updater.idle()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpnCBGLGRWSp"},"source":["!mv 1000_checkpoint.tar 6000_iters_good.tar\n","!cp 6000_iters_good.tar /content/drive/My\\ Drive/"],"execution_count":null,"outputs":[]}]}